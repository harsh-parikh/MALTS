# -*- coding: utf-8 -*-
"""MGH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1328EbQmeZ9maTWn5AGkikTTttcHecddR

# Setting up
"""

import sys
import os
#!{sys.executable} -m pip install keras-rl

import numpy as np
import pickle

import neuralnet 

#import torch
#import torch.nn as nn

#import gym
#from gym import spaces
#from gym.utils import seeding
#
#from keras.models import Sequential
#from keras.layers import Dense, Activation, Flatten
#from keras.optimizers import Adam
#
#from rl.agents.dqn import DQNAgent
#from rl.policy import EpsGreedyQPolicy
#from rl.memory import SequentialMemory

#import pickle
#
#from sklearn.ensemble import RandomForestRegressor as RFR
#from sklearn.svm import SVR
#
#import sklearn.linear_model as lm
#
import sklearn.neural_network as nn

#import torch
#import torch.nn as nn

datafile = open('../IICdata/preprocessed_dataset_all.pickle','rb')
data = pickle.load( datafile , encoding='latin')

big_datafile = open('../IICdata/for_Harsh/data/big_dataset.pickle','rb')
big_dataset = pickle.load( big_datafile , encoding='latin')

featurefile = open('../IICdata/for_Harsh/data/features_window_600.pickle','rb')
features = pickle.load( featurefile , encoding='latin')



"""# Q-Learning
Defining the model
"""



class QLearning:
    def __init__( self, state_space_dimension, action_space_dimension, covariate_space_dimension ):
        self.state_space_dimension = state_space_dimension
        self.action_space_dimension = action_space_dimension
        self.covariate_space_dimension = covariate_space_dimension
        self.Q = neuralnet.Net(self.state_space_dimension+self.action_space_dimension+self.covariate_space_dimension,1,[1000])
        self.A = neuralnet.Net(self.state_space_dimension+self.covariate_space_dimension,self.action_space_dimension,[500])
    def fit(self,features):
        Y = features['Y']
        C = features['C']
        E = features['E']
        D = features['D']
        n = len(Y)
        for itr in range(1000*n):
            i = np.random.randint(0,n)
            yi = Y[i]
            ci = C[i]
            Ei = E[i]
            Di = D[i]
            T,sd = Ei.shape
            _,ad = Di.shape
            t = np.random.randint(0,T)
            vec_t = np.array([ np.hstack( (Ei[t],Di[t],ci) ) ])
            if (t+1)<T and itr>100:
                vec_t1 = np.array([ np.hstack( (Ei[t+1],Di[t+1],ci) ) ])
                val_t1 = self.Q.predict(vec_t1)
            else:
                if yi == 0 or yi == 1 or yi == 2:
                    val_t1 = [ 10000/(yi+1) ]
                elif yi == 3:
                    val_t1 = [ 0 ]
                else:
                    val_t1 = [ -2000*yi ]
            self.Q.partial_fit(vec_t,val_t1)
        return self
    

Y = features['Y']
C = features['C']
E = features['E']
D = features['D']

QL = QLearning(0,0,0)
QL = QL.fit(features)

file = open('Q_outcome.csv','w') 
for i in range(0,1308):
    print(QL.Q.predict(np.array([ np.hstack( (E[i][-1],D[i][-1],C[i]) ) ]) )[0],Y[i],sep=', ',file=file)
file.close()

#action_dim = 34
#state_dim = 5
#log = open('log_mgh.log','w')
## Linear model Q(s,a) =  w[1,s,a]
#class LinearQ:
#    def __init__(self,state_dim,action_dim):
#        self.w = np.random.normal(0,5,1 + state_dim + action_dim )
#
#    def lin_Q(self,s,a):
#        vec = np.hstack(([1],s,a))
#        return np.dot(self.w,vec)
#
#    def loss(self,s,a,qt):
#        q = self.lin_Q(s,a)
#        return ( qt - q )**2
#    
#    def lin_gradient_loss(self,s,a,qt):
#        vec = np.hstack(([1],s,a))
#        q = self.lin_Q(s,a)
#        grad_w = -2*( qt - q )*vec
##        if not np.isnan(grad_w).all():
##            print( np.average(grad_w), self.loss(s,a,qt)  )
#        return grad_w
#    
#    def update_weights(self,grad_w,alpha=0.05):
#        self.w = self.w - alpha * grad_w
#        return self.w
#        
#    def fit(self,S,A,Y,R=None,gamma=1,alpha=0.05,iterations=10):
#        n = len(S)
#        for itr in range(0,iterations):
#            for i in range(0,n):
#                states = S[i][~np.isnan(S[i]).any(axis=1)]
#                actions = A[i][~np.isnan(S[i]).any(axis=1)]
#                T,ds = states.shape
#                _,da = actions.shape
#                reward = np.zeros((T,))
#                reward[T-1] = Y[i]
#                for t in range(T-1,-1,-1):
#                    if t<(T-1):
#                        qt = reward[t] + gamma*self.lin_Q(states[t+1,:],actions[t+1,:])
#                    elif t==(T-1):
#                        qt = reward[t]
#                    grad_w = self.lin_gradient_loss(states[t],actions[t],qt)
#                    self.w = self.update_weights(grad_w,alpha=alpha)
#        return self
#
#linQ = LinearQ( state_dim = state_dim, action_dim = action_dim )
#linQfitted = linQ.fit( S=data['E'], A=data['D'], Y=-data['Y'], alpha=0.0001 )
#
##def define_model(input_size,output_size):
##  model = Sequential()
##  model.add( Dense(units=20, input_shape=(input_size,) ) )
##  model.add( Activation('relu') )
##  model.add( Dense(output_size) )
##  model.add( Activation('linear') )
##  print( model.summary() )
##  return model
##
##model = define_model(1,nb_actions)
##
##"""Fitting the model"""
##
##policy = EpsGreedyQPolicy()
##memory = SequentialMemory(limit=50000, window_length=1)
##dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,
##target_model_update=1e-2, policy=policy)
##dqn.compile(Adam(lr=1e-3), metrics=['mae'])
##
##
##dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)
##
##dqn.test(env, nb_episodes=5, visualize=False)


"""# Naive Data Generation"""

#class StupidDrugRegime(gym.Env):
#  def __init__(self):
#      self.range = 1000  # +/- value the randomly select number can be between
#      self.bounds = 10  # Action space bounds
#
#      self.action_space = spaces.Discrete(self.bounds)
#      self.observation_space = spaces.Discrete(4)
#
#      self.number = 0
#      self.guess_count = 0
#      self.guess_max = 200
#      self.observation = 0
#
#      self.seed()
#      self.reset()
#
#  def seed(self, seed=None):
#      self.np_random, seed = seeding.np_random(seed)
#      return [seed]
#
#  def step(self, action):
#      assert self.action_space.contains(action)
#
#      if action < self.number:
#          self.observation = 3
#
#      elif action == self.number:
#          self.observation = 2
#
#      elif action > self.number:
#          self.observation = 1
#
#      reward = ((min(action, self.number) + self.bounds) / (max(action, self.number) + self.bounds)) ** 2
#
#      self.guess_count += 1
#      done = self.guess_count >= self.guess_max
#
#      return self.observation, reward, done, {"number": self.number, "guesses": self.guess_count}
#
#  def reset(self):
#      self.number = self.np_random.randint(0,self.bounds)
#      self.guess_count = 0
#      self.observation = 0
#      return self.observation
#
#
#"""Setting up the environment"""
#
#env = StupidDrugRegime()
#np.random.seed(123)
#env.seed(123)
#nb_actions = env.action_space.n




#class Neural_Network(nn.Module):
#    def __init__(self,inputSize,outputSize,hiddenSize=64):
#        super(Neural_Network, self).__init__()
#        # parameters
#        # TODO: parameters can be parameterized instead of declaring them here
#        self.inputSize = inputSize
#        self.outputSize = outputSize
#        self.hiddenSize = hiddenSize
#        
#        # weights
#        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor
#        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor
#        
#    def forward(self, X):
#        self.z = torch.matmul(X, self.W1) # 3 X 3 ".dot" does not broadcast in PyTorch
#        self.z2 = self.sigmoid(self.z) # activation function
#        self.z3 = torch.matmul(self.z2, self.W2)
#        o = self.sigmoid(self.z3) # final activation function
#        return o
#        
#    def sigmoid(self, s):
#        return 1 / (1 + torch.exp(-s))
#    
#    def sigmoidPrime(self, s):
#        # derivative of sigmoid
#        return s * (1 - s)
#    
#    def backward(self, X, o, error ):
#        self.o_error = error # error in output
#        self.o_delta = self.o_error * self.sigmoidPrime(o) # derivative of sig to error
#        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))
#        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)
#        self.W1 += torch.matmul(torch.t(X), self.z2_delta)
#        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)
#        
#    def train(self, X, y):
#        # forward + backward pass for training
#        o = self.forward(X)
#        error = y - o
#        self.backward(X, o, error )
#        
#    def saveWeights(self, model):
#        # we will use the PyTorch internal storage functions
#        torch.save(model, "NN")
#        # you can reload model with all the weights and so forth with:
#        # torch.load("NN")
#        
##    def predict(self):
##        print ("Predicted data based on trained weights: ")
##        print ("Input (scaled): \n" + str(xPredicted))
##        print ("Output: \n" + str(self.forward(xPredicted)))